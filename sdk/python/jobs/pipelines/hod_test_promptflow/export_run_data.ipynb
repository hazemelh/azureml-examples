{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02483ca7",
   "metadata": {},
   "source": [
    "\n",
    "# Export Promptflow Outputs\n",
    "\n",
    "This script is used for downloading all the run info needed and merge them together in local environments.\n",
    "\n",
    "\n",
    "## Prerequisite\n",
    "Make sure all required libraries are installed.Use command below to install azureml.core:\n",
    "\n",
    "    pip install azureml-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb56028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d5fdf",
   "metadata": {},
   "source": [
    "## Get workspace\n",
    "Get gonnections to workspace with personal token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "879fe8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "# Get a logger which allows us to log events that occur when running the program.\n",
    "logger = logging.getLogger(\"myLogger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter(\"%(asctime)s %(message)s\")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "ch.setLevel(logging.DEBUG)\n",
    "if not logger.handlers:\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "# Get the workspace associated with your personal token and get the relavant datastore.\n",
    "subscription_id = \"96aede12-2f73-41cb-b983-6d11a904839b\"\n",
    "resource_group = \"promptflow\"\n",
    "workspace_name = \"promptflow-eastus\"\n",
    "ws = Workspace.get(workspace_name, subscription_id=subscription_id, resource_group=resource_group)\n",
    "region = ws.location\n",
    "default_datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0d016",
   "metadata": {},
   "source": [
    "## Define utils\n",
    "Define functions for downloading and loading run info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef46110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run info for the given run_id\n",
    "def get_run_info(run_id):\n",
    "    logger.info(f\"Getting Run Info for Run: {run_id}\")\n",
    "    run = ws.get_run(run_id=run_id)\n",
    "    display_name = run.display_name\n",
    "    input_run_id = run.properties.get('azureml.promptflow.input_run_id')\n",
    "    return display_name, input_run_id\n",
    "\n",
    "# Get the output asset id for the given run_id and asset_name\n",
    "def get_output_asset_id(run_id, asset_name):\n",
    "    logger.info(f\"Getting Output Asset Id for Run {run_id}\")\n",
    "    if region == \"centraluseuap\":\n",
    "        url = f\"https://int.api.azureml-test.ms/history/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/rundata\"\n",
    "    else:\n",
    "        url = f\"https://ml.azure.com/api/{region}/history/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/rundata\"\n",
    "    payload = {\n",
    "        \"runId\": run_id,\n",
    "        \"selectRunMetadata\": True\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=ws._auth.get_authentication_header())\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to get output asset id for run {run_id} because RunHistory API returned status code {response.status_code}. Response: {response.text}\")\n",
    "    output_asset_id = response.json()[\"runMetadata\"][\"outputs\"][asset_name][\"assetId\"]\n",
    "    return output_asset_id\n",
    "\n",
    "# Get the asset path for the given asset_id\n",
    "def get_asset_path(asset_id):\n",
    "    logger.info(f\"Getting Asset Path for Asset Id {asset_id}\")\n",
    "    if region == \"centraluseuap\":\n",
    "        url = f\"https://int.api.azureml-test.ms/data/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/dataversion/getByAssetId\"\n",
    "    else:\n",
    "        url = f\"https://ml.azure.com/api/{region}/data/v1.0/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/dataversion/getByAssetId\"\n",
    "    payload = {\n",
    "        \"value\": asset_id,\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=ws._auth.get_authentication_header())\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Failed to get asset path for asset id {asset_id} because Data API returned status code {response.status_code}. Response: {response.text}\")\n",
    "    data_uri = response.json()[\"dataVersion\"][\"dataUri\"]\n",
    "    relative_path = data_uri.split(\"/paths/\")[-1]\n",
    "    return relative_path\n",
    "\n",
    "# Get the flow artifact relative path for the given run_id\n",
    "def get_flow_artifact_relative_path(run_id):\n",
    "    logger.info(f\"Getting Flow Artifact Relative Path for Run {run_id}\")\n",
    "    try:\n",
    "        flow_artifact_asset_id = get_output_asset_id(run_id, \"debug_info\")\n",
    "        relative_path = get_asset_path(flow_artifact_asset_id)\n",
    "        relative_path += \"flow_artifacts/\"\n",
    "        return relative_path\n",
    "    except Exception as e:\n",
    "        logger.warning(\"`debug_info` output assets is not available, maybe because the job ran on old version runtime, trying to get `flow_outputs` output asset instead.\")\n",
    "        output_asset_id = get_output_asset_id(run_id, \"flow_outputs\")\n",
    "        relative_path = get_asset_path(output_asset_id)\n",
    "        return relative_path.replace(\"flow_outputs\", \"flow_artifacts\")\n",
    "\n",
    "# Download flow artifacts for the given run_id\n",
    "def download_flow_artifacts(run_id, blob_prefix):\n",
    "    logger.info(f\"Downloading Flow Artifacts for Run {run_id}\")\n",
    "    target_dir = f\"./downloads/{run_id}\"\n",
    "    default_datastore.download(target_dir, prefix=blob_prefix, overwrite=True)\n",
    "    return target_dir\n",
    "\n",
    "# Load flow artifacts of the run from the given local_dir\n",
    "def load_flow_artifacts(local_dir, run_display_name):\n",
    "    logger.info(f\"Loading Flow Artifacts of {run_display_name} from {local_dir}\")\n",
    "    flow_artifacts = {}\n",
    "    p = Path(local_dir)\n",
    "    for file_path in p.glob(\"**/*.jsonl\"):\n",
    "        with open(file_path) as fp:\n",
    "            for line in fp:\n",
    "                line_record = json.loads(line)\n",
    "                line_number = line_record.get(\"line_number\")\n",
    "                run_info = line_record.get(\"run_info\") or {}\n",
    "                status = run_info.get(\"status\")\n",
    "                outputs = run_info.get(\"output\") or {}\n",
    "                inputs = run_info.get(\"inputs\") or {}\n",
    "                modified_inputs = {f\"inputs.{k}\": v for k, v in inputs.items()}\n",
    "                record = {\n",
    "                    \"Line number\": line_number,\n",
    "                    \"Run\": run_display_name,\n",
    "                    \"Status\": status,\n",
    "                    **modified_inputs,\n",
    "                    **outputs\n",
    "                }\n",
    "                flow_artifacts[line_number] = record\n",
    "    return flow_artifacts\n",
    "\n",
    "# Get required info for the given run_id\n",
    "def get_required_info(run_id):\n",
    "    logger.info(f\"Processing Run: {run_id}\")\n",
    "    display_name, input_run_id = get_run_info(run_id)\n",
    "    flow_artifact_relative_path = get_flow_artifact_relative_path(run_id)\n",
    "\n",
    "    target_dir = download_flow_artifacts(run_id, flow_artifact_relative_path)\n",
    "    flow_artifacts = load_flow_artifacts(target_dir, display_name)\n",
    "    return {\n",
    "        \"run_id\": run_id,\n",
    "        \"display_name\": display_name,\n",
    "        \"input_run_id\": input_run_id,\n",
    "        \"flow_artifacts\": flow_artifacts\n",
    "    }\n",
    "\n",
    "# Update downstream flow artifacts with the given display_name\n",
    "def update_downstread_flow_artifacts(flow_artifacts, display_name):\n",
    "    updated_flow_artifacts = {}\n",
    "    for line_number, line_record in flow_artifacts.items():\n",
    "        updated_line_record = {}\n",
    "        for k, v in line_record.items():\n",
    "            if k == \"Run\" or k == \"Status\" or k == \"Line number\":\n",
    "                continue\n",
    "            else:\n",
    "                updated_line_record[f\"{k}({display_name})\"] = v\n",
    "        updated_flow_artifacts[line_number] = updated_line_record\n",
    "    return updated_flow_artifacts\n",
    "\n",
    "# Merge flow artifacts for the given run_infos, joining flow artifacts with the same line number\n",
    "def merge_flow_artifacts(run_infos: list):\n",
    "    run_info_dict = {run_info[\"run_id\"]: run_info for run_info in run_infos}\n",
    "    main_flow_infos = []\n",
    "    for run_id, run_info in run_info_dict.items():\n",
    "        if run_info[\"input_run_id\"] is None:\n",
    "            main_flow_infos.append(run_info)\n",
    "        else:\n",
    "            input_run_id = run_info[\"input_run_id\"]\n",
    "            if input_run_id in run_info_dict:\n",
    "                input_run_info = run_info_dict[input_run_id]\n",
    "                updated_flow_artifacts = update_downstread_flow_artifacts(run_info[\"flow_artifacts\"], run_info[\"display_name\"])\n",
    "                for line_number, updated_flow_artifact in updated_flow_artifacts.items():\n",
    "                    if line_number in input_run_info[\"flow_artifacts\"]:\n",
    "                        input_run_info[\"flow_artifacts\"][line_number].update(updated_flow_artifact)\n",
    "            else:\n",
    "                # Input Run is not included, treat this run as main flow\n",
    "                main_flow_infos.append(run_info)\n",
    "    \n",
    "    merge_result = []\n",
    "    for main_flow_info in main_flow_infos:\n",
    "        merge_result += list(main_flow_info[\"flow_artifacts\"].values())\n",
    "    merge_result = sorted(merge_result, key=lambda x: f\"{x['Line number']}{x['Run']}\")\n",
    "    return merge_result\n",
    "\n",
    "# Convert the given jsonl file to csv file\n",
    "def jsonl_to_csv(jsonl_path, csv_path):\n",
    "    max_keys = []\n",
    "    with open(jsonl_path, \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            row = json.loads(line)\n",
    "            if len(row.keys()) > len(max_keys):\n",
    "                max_keys = list(row.keys())\n",
    "\n",
    "    with open(jsonl_path, \"r\") as jsonl_file:\n",
    "        with open(csv_path, \"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow(max_keys)\n",
    "            for line in jsonl_file:\n",
    "                row = json.loads(line)\n",
    "                csv_row = [row.get(key) for key in max_keys]\n",
    "                writer.writerow(csv_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3a3005",
   "metadata": {},
   "source": [
    "## Download runs\n",
    "Download jsonl of the runs, merge them and save to local jsonl_path and csv_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c80773b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1116290924.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    run_ids = [<your-run-link-list>]\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# The run_ids include all the runs selected from the UI.\n",
    "run_ids = [<your-run-link-list>]\n",
    "\n",
    "# Download the flow artifacts from the run_ids and merge them into a single file.\n",
    "deduped_run_ids = list(set(run_ids))\n",
    "run_infos = [get_required_info(run_id) for run_id in deduped_run_ids]\n",
    "merged_result = merge_flow_artifacts(run_infos)\n",
    "\n",
    "# Save the merged result as jsonl and csv.\n",
    "jsonl_path = \"./merged_result.jsonl\"\n",
    "csv_path = \"./merged_result.csv\"\n",
    "with open(jsonl_path, \"w\") as fp:\n",
    "    for record in merged_result:\n",
    "        fp.write(json.dumps(record))\n",
    "        fp.write(\"\\n\")\n",
    "logger.info(f\"Saved merged result as jsonl to {jsonl_path}\")\n",
    "jsonl_to_csv(jsonl_path, csv_path)\n",
    "logger.info(f\"Saved merged result as csv to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a9028",
   "metadata": {},
   "source": [
    "## Visualize data\n",
    "Load local csv file and visualize it as pandas dataframe. Make sure you have installed pandas:\n",
    "\n",
    "    pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4e707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged result as pandas dataframe and sort it by line number.\n",
    "import pandas as pd\n",
    "data = pd.read_csv('merged_result.csv', dtype={'Line number': 'int64'})\n",
    "df = pd.DataFrame(data)\n",
    "df.sort_values(by='Line number', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66603fa",
   "metadata": {},
   "source": [
    "## Potential Problems and Solutions\n",
    "  \n",
    "### 1. [Winerror] File name or path too long/The system cannot find the path specified \n",
    "\n",
    "**Solution: Enable NTFS long paths in Windows**\n",
    "\n",
    "**For Windows Home Users: Filesystem Registry entry**\n",
    "1. Press `Win + R` keys on your keyboard and type `regedit` then press `Enter`. The Registry Editor will be opened.\n",
    "2. Go to `HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem`.\n",
    "3. Create a new 32-bit DWORD value named `LongPathsEnabled` and set it to `1`.\n",
    "4. Restart your Windows.\n",
    "\n",
    "**For Windows Pro and Enterprise (Windows Server 2016 or above) Users: Using Group Policy**\n",
    "1. Press `Win + R` keys on your keyboard and type `gpedit.msc` then press `Enter`. The Group Policy Editor will be opened.\n",
    "2. Go to `Local Computer Policy -> Computer Configuration -> Administrative Templates -> System -> Filesystem`, then enable the `Enable Win32 long paths` option.\n",
    "3. Restart your Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272a59e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 15:10:51,993 Getting Asset Path for Asset Id azureml://locations/eastus/workspaces/3e123da1-f9a5-4c91-9234-8d9ffbb39ff5/data/azureml_c619f648-c809-4545-9f94-f67b0a680706_output_data_debug_info/versions/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/\n"
     ]
    }
   ],
   "source": [
    "path = get_asset_path(\"azureml://locations/eastus/workspaces/3e123da1-f9a5-4c91-9234-8d9ffbb39ff5/data/azureml_c619f648-c809-4545-9f94-f67b0a680706_output_data_debug_info/versions/1\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-30 15:15:12,750 Downloading Flow Artifacts for Run c619f648-c809-4545-9f94-f67b0a680706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/flow_artifacts/000000000_000000024.jsonl\n",
      "Downloaded promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/flow_artifacts/000000000_000000024.jsonl, 1 files out of an estimated total of 5\n",
      "Downloading promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/flow_outputs/output.jsonl\n",
      "Downloaded promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/flow_outputs/output.jsonl, 2 files out of an estimated total of 5\n",
      "Downloading promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/instance_results.jsonl\n",
      "Downloaded promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/instance_results.jsonl, 3 files out of an estimated total of 5\n",
      "Downloading promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/meta.json\n",
      "Downloaded promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/meta.json, 4 files out of an estimated total of 5\n",
      "Downloading promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/node_artifacts/hello_world/000000000.jsonl\n",
      "Downloaded promptflow/PromptFlowArtifacts/c619f648-c809-4545-9f94-f67b0a680706/node_artifacts/hello_world/000000000.jsonl, 5 files out of an estimated total of 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./downloads/c619f648-c809-4545-9f94-f67b0a680706'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_flow_artifacts(\"c619f648-c809-4545-9f94-f67b0a680706\", path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
