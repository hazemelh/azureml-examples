{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat group examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent\n",
    "\n",
    "copiplot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "# for eager flow, may use below\n",
    "# simulation_agent = ChatAgent(name=\"simulation_flow\", eager_flow=entry_func)\n",
    "\n",
    "with ChatGroup(\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    entry_agent=copiplot_agent,  # the first agent to speak\n",
    "    speak_order=[copiplot_agent, simulation_agent],  # other than specifying, can also be \"LLM\" or \"AUTO\"(default)\n",
    ") as chat_group:\n",
    "    copiplot_agent.io_mapping(\n",
    "        question=simulation_agent.outputs.generated_question, \n",
    "        model=\"gpt4\",\n",
    "    )\n",
    "    simulation_agent.io_mapping(\n",
    "        persona=\"Tom\", \n",
    "        last_answer=copiplot_agent.outputs.output,\n",
    "        chat_history=chat_group.chat_history,  # chat_history is a group-level context\n",
    "        model=\"gpt4\",\n",
    "        goal=data_column,  # not sure the original purpose\n",
    "    )\n",
    "    # simulation_agent.terminate_func = lambda x: x == \"<END>\"\n",
    "\n",
    "    # kick off the chat\n",
    "    chat_group.run()\n",
    "\n",
    "    # access to the agent outputs\n",
    "    print(\"Chat history:\", chat_group.chat_history)\n",
    "    print(\"Last copilot flow output:\", copiplot_agent.outputs[\"output\"])\n",
    "    print(\"Last simulation flow output:\", simulation_agent.outputs[\"generated_question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDK option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent\n",
    "\n",
    "copiplot_agent = ChatAgent(name=\"copilot_flow\", flow=\"./promotflow_copilot\")\n",
    "simulation_agent = ChatAgent(name=\"simulation_flow\", flow=\"./promptflow_simulation\")\n",
    "\n",
    "chat_group = ChatGroup(\n",
    "    agents=[copiplot_agent, simulation_agent],\n",
    "    max_turns=10,\n",
    "    max_token=5000,\n",
    "    max_time=600,  # 10 minutes\n",
    "    entry_agent=copiplot_agent,  # the first agent to speak\n",
    "    speak_order=[copiplot_agent, simulation_agent],  # other than specifying, can also be \"LLM\" or \"AUTO\"(default)\n",
    "    io_mapping = {\n",
    "        \"copilot_flow.question\": \"${simulation_flow.outputs.generated_question}\",\n",
    "        \"copilot_flow.model\": \"gpt4\",  # model is an external parameter\n",
    "        \"simulation_flow.persona\": \"Tom\", \n",
    "        \"simulation_flow.chat_history\": \"${group.chat_history}\",  # chat_history is a group-level context\n",
    "        \"simulation_flow.model\": \"gpt4\",\n",
    "        \"simulation_flow.goal\": \"<jsonl_data.column1>\",  # not sure the original purpose\n",
    "    }\n",
    ")\n",
    "\n",
    "# kick off the chat\n",
    "chat_group.run()\n",
    "\n",
    "# access to the agent outputs\n",
    "print(\"Chat history:\", chat_group.chat_history)\n",
    "print(\"Last copilot flow output:\", copiplot_agent.outputs[\"output\"])\n",
    "print(\"Last simulation flow output:\", simulation_agent.outputs[\"generated_question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original yaml in experiment\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 5\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "        inputs:\n",
    "          question: ${roles.simulation_flow.outputs.generated_question}\n",
    "          chat_history: []\n",
    "          model: ${inputs.model_name}\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "        inputs:\n",
    "          persona: ${inputs.persona}\n",
    "          model: ${inputs.model_name}\n",
    "          goal: ${data.leo_min_set.query}\n",
    "          chat_history: ${group.chat_history}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option 1\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 10\n",
    "    max_token: 5000\n",
    "    max_time: 600  # 10 minutes\n",
    "    entry_agent: copilot_flow\n",
    "    speak_order: [copilot_flow, simulation_flow]\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "        inputs:\n",
    "          question: ${roles.simulation_flow.outputs.generated_question}\n",
    "          chat_history: []\n",
    "          model: ${inputs.model_name}\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "        inputs:\n",
    "          persona: ${inputs.persona}\n",
    "          model: ${inputs.model_name}\n",
    "          goal: ${data.leo_min_set.query}\n",
    "          chat_history: ${group.chat_history}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yaml option 2\n",
    "\n",
    "```yaml\n",
    "# multi turn conversation is described as a chat group, which contains a copilot flow and question simulation flow\n",
    "  - name: multi_turn_chat\n",
    "    type: chat_group\n",
    "    max_turns: 10\n",
    "    max_token: 5000\n",
    "    max_time: 600  # 10 minutes\n",
    "    agents:\n",
    "      - name: copilot_flow\n",
    "        flow: ../copilot/promotflow_copilot/flow.dag.yaml\n",
    "      - name: simulation_flow\n",
    "        flow: ../evaluation/similarity.yaml\n",
    "    entry_agent: copilot_flow\n",
    "    speak_order: [copilot_flow, simulation_flow]\n",
    "    io_mapping:\n",
    "        copilot_flow.question: ${simulation_flow.outputs.generated_question}\n",
    "        copilot_flow.model: ${inputs.model_name}\n",
    "        copilot_flow.chat_history: []  # maybe omit?\n",
    "        simulation_flow.persona: ${inputs.persona}\n",
    "        simulation_flow.chat_history: ${group.chat_history}  # chat_history is a group-level context\n",
    "        simulation_flow.model: ${inputs.model_name}\n",
    "        simulation_flow.goal: ${data.leo_min_set.query}  # not sure the original purpose\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatAgent\n",
    "\n",
    "def terminate_chat(result):\n",
    "    # result is the flow result that should be a dict\n",
    "    return result[\"score\"] >= 10\n",
    "\n",
    "simulation_agent = ChatAgent(\n",
    "    name=\"simulation_flow\", \n",
    "    flow=\"./promptflow_simulation\",\n",
    "    terminate_func=terminate_chat,  # set customized terminate function\n",
    ")\n",
    "\n",
    "# this is the default terminate function if not specified\n",
    "def default_terminate_function(result):\n",
    "    # this would require the flow result has only one output, not a dict\n",
    "    return result[\"output\"] == \"<END>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "[\n",
    "    [\"copilot_agent\", \"chat message 1\"],\n",
    "    [\"simulation_agent\", \"chat message 2\"],\n",
    "    [\"copilot_agent\", \"chat message 3\"],\n",
    "]\n",
    "\n",
    "# Option 2\n",
    "[\n",
    "    \"<copilot_agent>: chat message 1\",\n",
    "    \"<simulation_agent>: chat message 2\",\n",
    "    \"<copilot_agent>: chat message 3\",\n",
    "]\n",
    "\n",
    "# or maybe save both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "456\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"123\")\n",
    "finally:\n",
    "    print(\"456\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow import ChatGroup, ChatAgent, ChatManager\n",
    "\n",
    "manager = ChatManager(template=\"template.jinja2\", connection=\"<connection_name>\")\n",
    "chat_group = ChatGroup(manager=manager, max_turns=10, max_token=5000, max_time=600, ...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
